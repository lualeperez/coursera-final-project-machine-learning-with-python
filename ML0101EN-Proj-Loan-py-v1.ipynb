{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "source": "<a href=\"https://www.bigdatauniversity.com\"><img src=\"https://ibm.box.com/shared/static/cw2c7r3o20w9zn8gkecaeyjhgw3xdgbj.png\" width=\"400\" align=\"center\"></a>\n\n<h1 align=\"center\"><font size=\"5\">Classification with Python</font></h1>"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "source": "In this notebook we try to practice all the classification algorithms that we learned in this course.\n\nWe load a dataset using Pandas library, and apply the following algorithms, and find the best one for this specific dataset by accuracy evaluation methods.\n\nLets first load required libraries:"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "outputs": [],
            "source": "import itertools\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import NullFormatter\nimport pandas as pd\nimport numpy as np\nimport matplotlib.ticker as ticker\nfrom sklearn import preprocessing\n%matplotlib inline"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "source": "### About dataset"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "source": "This dataset is about past loans. The __Loan_train.csv__ data set includes details of 346 customers whose loan are already paid off or defaulted. It includes following fields:\n\n| Field          | Description                                                                           |\n|----------------|---------------------------------------------------------------------------------------|\n| Loan_status    | Whether a loan is paid off on in collection                                           |\n| Principal      | Basic principal loan amount at the                                                    |\n| Terms          | Origination terms which can be weekly (7 days), biweekly, and monthly payoff schedule |\n| Effective_date | When the loan got originated and took effects                                         |\n| Due_date       | Since it\u2019s one-time payoff schedule, each loan has one single due date                |\n| Age            | Age of applicant                                                                      |\n| Education      | Education of applicant                                                                |\n| Gender         | The gender of applicant                                                               |"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "source": "Lets download the dataset"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "outputs": [],
            "source": "!wget -O loan_train.csv https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/ML0101ENv3/labs/loan_train.csv"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "source": "### Load Data From CSV File  "
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Define a function to get the data from a csv file, and to remove dummy columns (with name Unnamed)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "def get_data(file):\n    \"\"\"\n    Get the data from a data file in csv format\n    \"\"\"\n    \n    df = pd.read_csv(file)\n    cols_to_drop = [] # list with column names to remove\n    # 1st check that the columns names are in the data\n    cols_to_test = ['Unnamed: 0','Unnamed: 0.1']\n    for col in cols_to_test:\n        if col in df.columns:\n            cols_to_drop.append(col)\n            \n    if cols_to_drop:\n        df.drop(columns=cols_to_drop,axis=1,inplace=True)\n    \n    return df\n"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "outputs": [],
            "source": "df = get_data('loan_train.csv')\ndf.head()"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "df.shape"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# Will use the copy (df_for_fit) for later when training the data\n# Will use df to explore the data\ndf_for_fit = df.copy()\ndf_for_fit.shape"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "source": "### Convert to date time object "
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "outputs": [],
            "source": "df['due_date']       = pd.to_datetime(df['due_date'])\ndf['effective_date'] = pd.to_datetime(df['effective_date'])\ndf.head()"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "source": "# Data visualization and pre-processing\n\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "source": "Let\u2019s see how many of each class is in our data set "
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "outputs": [],
            "source": "df['loan_status'].value_counts()"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "source": "260 people have paid off the loan on time while 86 have gone into collection \n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Lets plot some columns to underestand data better:"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# notice: installing seaborn might takes a few minutes\n!conda install -c anaconda seaborn -y"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "import seaborn as sns"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "def nice_plot(df,\n              x_var,\n              minmax = None,\n              col = 'Gender',\n              hue = 'loan_status',\n              pos_label = 'PAIDOFF',\n              anbins = 10,\n              afigsize    = (7,3),\n              afontsize   = 6,\n             ):\n    \n    nbins      = anbins\n    figsize    = afigsize\n    fontsize   = afontsize\n\n    # x_var = \"Principal\"\n    xmin    = None\n    xmax    = None\n    percent = 0.001\n    if minmax is None:\n        xmin,xmax = df[x_var].min(),df[x_var].max()\n    else:\n        xmin,xmax = minmax\n        percent   = 0.0\n        nbins     = int(xmax - xmin)\n        \n    delta     = xmax - xmin\n    xmin     -= percent*delta\n    xmax     += percent*delta\n\n    bins = np.linspace(xmin, xmax, nbins+1)\n    g = sns.FacetGrid(df, col=col, hue=hue, palette=\"Set1\", col_wrap=2)\n    g.map(plt.hist, x_var, bins=bins, ec=\"k\")\n\n    g.axes[-1].legend()\n    plt.show()\n\n\n    fig, ax = plt.subplots(nrows=1, ncols=2, figsize=figsize)\n    gender_list = list(df[col].unique())\n    for idx,gen in enumerate(gender_list):\n        x_values = []\n        y_values = []\n        for i in range(len(bins)-1):\n            cond_x    = (df[x_var] >= bins[i]) & (df[x_var] < bins[i+1])\n            cond_good = df[hue] == pos_label\n            cond_gend = df[col] == gen\n            cond_all= cond_x & cond_gend\n            x_values.append(0.5*(bins[i] + bins[i+1]))\n            Ngood = df[(cond_all) & (cond_good)].shape[0]\n            Nall  = df[(cond_all)].shape[0]\n            if Nall == 0:\n                y_values.append(-1)\n            else:\n                y_values.append(100*Ngood/Nall)\n\n        ax[idx].errorbar(x = np.array(x_values),\n                         y = np.array(y_values),\n                         yerr = 0,\n                         fmt  = \"bo\",\n                         linewidth  = 3,\n                         markersize = 4,\n                        )\n\n        ax[idx].set_xlim([bins[0],bins[-1]])\n        ax[idx].set_ylim([0,110])\n        ax[idx].set_xlabel(x_var, fontsize=fontsize)\n        ax[idx].set_ylabel(pos_label + ' frac (%)', fontsize=fontsize)\n        ax[idx].set_title(col + ' = ' + gen, fontsize=fontsize)\n\n    plt.show()\n    "
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "nice_plot(df = df,\n          x_var = \"Principal\",\n          minmax = None,\n          col = 'Gender',\n          hue = 'loan_status',\n          pos_label = 'PAIDOFF',\n          anbins = 10,\n          afigsize    = (7,3),\n          afontsize   = 8)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "outputs": [],
            "source": "nice_plot(df = df,\n          x_var = \"age\",\n          minmax = None,\n          col = 'Gender',\n          hue = 'loan_status',\n          pos_label = 'PAIDOFF',\n          anbins = 10,\n          afigsize    = (7,3),\n          afontsize   = 8)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "It doesn't seem to be a correlation between the age and the likelihood of paying a loan"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "source": "# Pre-processing:  Feature selection/extraction"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "source": "### Lets look at the day of the week people get the loan "
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "df['dayofweek'] = df['effective_date'].dt.dayofweek"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "outputs": [],
            "source": "nice_plot(df = df,\n          x_var = \"dayofweek\",\n          minmax = (0.0-0.5,6+0.5),\n          col = 'Gender',\n          hue = 'loan_status',\n          pos_label = 'PAIDOFF',\n          anbins = 10,\n          afigsize    = (7,3),\n          afontsize   = 8)"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "source": "We see that people who get the loan at the end of the week dont pay it off, so lets use Feature binarization to set a threshold values less then day 4 "
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "outputs": [],
            "source": "df['weekend'] = df['dayofweek'].apply(lambda x: 1 if (x>3)  else 0)\ndf.head()"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### Lets look at the day of the week when people have to pay the loan "
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "df['dayofweek_pay'] = df['due_date'].dt.dayofweek"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "nice_plot(df = df,\n          x_var = \"dayofweek_pay\",\n          minmax = (0.0-0.5,6+0.5),\n          col = 'Gender',\n          hue = 'loan_status',\n          pos_label = 'PAIDOFF',\n          anbins = 10,\n          afigsize    = (7,3),\n          afontsize   = 8)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "It seems that people miss to pay the loan when the due date is either the begining of the week or the weekend"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "### Visualizing the time people have  to paye the loan"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "factor_sec_to_days = 1./(24*3600.)\ndf['time_to_pay_days'] = (df['due_date'] - df['effective_date']).dt.total_seconds()*factor_sec_to_days\ndf.head()"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "nice_plot(df = df,\n          x_var = \"time_to_pay_days\",\n          minmax = None,\n          col = 'Gender',\n          hue = 'loan_status',\n          pos_label = 'PAIDOFF',\n          anbins = 10,\n          afigsize    = (7,3),\n          afontsize   = 8)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "It seems that longer the due time for paying the loan, less likely are the people able to pay it.  \nLets look at the correlation between Principal and time_to_pay_days."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "figsize    = (7,3)\nfontsize   = 8\n    \nx_var = 'Principal'\ny_var = 'time_to_pay_days'\n\nxmin,xmax = df[x_var].min(),df[x_var].max()\nymin,ymax = df[y_var].min(),df[y_var].max()\n\npercent   = 0.01\ndelta     = xmax - xmin\nxmin     -= percent*delta\nxmax     += percent*delta\n\ndelta     = ymax - ymin\nymin     -= percent*delta\nymax     += percent*delta\nymin      = 0.0\n\n# bins = np.linspace(xmin, xmax, nbins+1)\n\nfig, ax = plt.subplots(nrows=1, ncols=2, figsize=figsize)\ngender_list = list(df['Gender'].unique())\nfor idx,gen in enumerate(gender_list):\n    cond = df['Gender'] == gen\n    ax[idx].scatter(x = df.loc[cond,x_var],\n                    y = df.loc[cond,y_var],\n                    c = 'b',\n                    # linewidth  = 3,\n                    # markersize = 4,\n                   )\n\n    ax[idx].set_xlim([xmin,xmax])\n    ax[idx].set_ylim([ymin,ymax])\n    ax[idx].set_xlabel(x_var, fontsize=fontsize)\n    ax[idx].set_ylabel(y_var, fontsize=fontsize)\n    ax[idx].set_title('Gender = ' + gen, fontsize=fontsize)\n\nplt.show()"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "source": "## Convert Categorical features to numerical values"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "source": "Lets look at gender:"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "outputs": [],
            "source": "df.groupby(['Gender'])['loan_status'].value_counts(normalize=True)"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "source": "86 % of female pay there loans while only 73 % of males pay there loan\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "source": "Lets convert male to 0 and female to 1:\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "source": "How about education?"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "outputs": [],
            "source": "df.groupby(['education'])['loan_status'].value_counts(normalize=True)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "It seems that there isn't a big correlation with the education level"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "source": "# Install some libraries to be used when building the estimator pipelines "
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "!conda update setuptools"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# notice: installing dask-ml might takes a few minutes\n!conda install -c anaconda dask-ml=0.12.0 -y"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "from dask_ml.preprocessing import Categorizer, DummyEncoder, StandardScaler"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Automatise preprocessing\nPut all the global preprocessing steps into a single function"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "def do_global_preprocessing(df):\n    \"\"\" \n    This function sumarizes all the global preprocessing steps defined in the cells above \n    for exploration data analysis (data cleaning and feature engineering)\n    \"\"\"\n    \n    # Copy the data\n    df_copy = df.copy()\n    \n    # Convert data columns to datatime format\n    data_cols = [c for c in df_copy.columns if \"date\" in c]\n    for col in data_cols:\n        df_copy[col] = pd.to_datetime(df_copy[col])\n    \n    # Generate the dayofweek feature\n    df_copy['dayofweek'] = df_copy['effective_date'].dt.dayofweek\n    \n    # Generate the weekend feature\n    df_copy['weekend'] = df_copy['dayofweek'].apply(lambda x: 1 if (x>3)  else 0)\n    \n    # Generate the dayofweek_pay feature\n    df_copy['dayofweek_pay'] = df_copy['due_date'].dt.dayofweek\n    # Generate the middle_week_pay feature\n    df_copy['middle_week_pay'] = df_copy['dayofweek_pay'].apply(lambda x: 1 if (x == 0 or x>3)  else 0)\n    \n    # Generate the time_to_pay_days feature\n    # This is the time the clients have to pay the loan, i.e due_date - effective_data\n    factor_sec_to_days = 1./(24*3600.)\n    df_copy['time_to_pay_days'] = (df_copy['due_date'] - df_copy['effective_date']).dt.total_seconds()*factor_sec_to_days\n    \n    # Convert the target to binary variable\n    df_copy['loan_status'].replace(to_replace=['PAIDOFF','COLLECTION'], value=[1,0],inplace=True)\n    \n    return df_copy\n    "
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# Show the data previous to the global preprocessing\ndf_for_fit.head()"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# Preprocess the data and show it\ndf_for_fit_prep = do_global_preprocessing(df_for_fit)\ndf_for_fit_prep.head()"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# Now define the training data, serparating the predictors and the target\nX_train = df_for_fit_prep\ny_train = X_train.pop(\"loan_status\")"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "source": "### Feature selection"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "source": "Lets define a class sklearn pipeline compatible to select the features for model training"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "from sklearn.base import BaseEstimator, TransformerMixin\n\nclass FeatureSelector(BaseEstimator, TransformerMixin):\n    \"\"\"\n    Select a list of features from data\n\n    parameters:\n    -----------\n      columns: str or list of str, list of columns to select\n    \"\"\"\n\n    def __init__(self, columns=None):\n        # list of columns specified to be selected\n        self.columns = columns\n\n    def get_cols(self,X):\n        \"\"\" Build the list of columns to select \"\"\"\n\n        cols = []\n        # check if the list of columns specified by the user is in data list of columns\n        if self.columns is None:\n            return cols\n\n        cols = list(set(X.columns).intersection(set(self.columns)))\n\n        return cols\n\n    def fit(self, X, y=None):\n        # Add the possibility to give as input a string instead of a list\n        if isinstance(self.columns,list):\n            pass\n        elif isinstance(self.columns,str):\n            # if the input is a string converted to the list with a single element\n            self.columns = [self.columns]\n        else:\n            raise ValueError(\"FeatureSelector (fit): columns parameter has to be either a string or a list of strings.\")\n\n        self.selected_columns = self.get_cols(X)\n            \n        return self\n\n    def transform(self, X, y=None):\n        if len(self.selected_columns) == 0:\n            # If selected columns is empy do nothing\n            return X\n        else:\n            # Use only the columns in the data in the selected_columns list\n            return  X.loc[:,self.selected_columns]\n    "
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "source": "# Classification "
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "source": "Now, it is your turn, use the training set to build an accurate model. Then use the test set to report the accuracy of the model\nYou should use the following algorithm:\n- K Nearest Neighbor(KNN)\n- Decision Tree\n- Support Vector Machine\n- Logistic Regression\n\n\n\n__ Notice:__ \n- You can go above and change the pre-processing, feature selection, feature-extraction, and so on, to make a better model.\n- You should use either scikit-learn, Scipy or Numpy libraries for developing the classification algorithms.\n- You should include the code of the algorithm in the following cells."
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "# Preprocessing pipeline"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "Will start by defining a couple of functions to be used in the process of model optimization, which is grid-search CV"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "The function below counts the number of parameters combinations that are going to be tested during grid-search CV"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "def count_number_of_fits(grid_params):\n    \"\"\"\n    Count the number of parameter combinations to be tested during grid-search CV\n    \n    parameters\n    ----------\n       grid_params: a dictionary with the grid values of the parameters\n    \"\"\"\n    \n    nfits = 1\n    for k,v in grid_params.items():\n        nfits *= len(v)\n        \n    return nfits\n"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "The function below prints out a summary of the grid-search CV optimization process"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "def  summary_results(gscv,name):\n    \"\"\"\n    Print summary of results from grid search CV\n    \"\"\"\n    \n    msg = '\\n{0:-^31}\\n'.format('Summary {}'.format(name))\n\n    cv_res = gscv.cv_results_\n    \n    row = gscv.best_index_\n\n    # print best estimator metrics\n    msg += '\\n{0:>20s}\\n'.format('# Metrics of best estimator:')\n    msg += '{0:_>35}\\n'.format('')\n\n    msg += '{0:>20s}:{1:>10d}\\n'.format('Estimator index',row)\n    \n    param_refit_scorer = gscv.get_params().get('refit')\n    if type(param_refit_scorer) is str:\n        msg += '{0:>20s}:{1:>10s}\\n'.format('Refit scorer',param_refit_scorer)\n\n    refit_time_value = gscv.refit_time_\n    suffix = 'sec'\n    msg += '{0:>20s}:{1:>10.5f}{2:>4s}\\n'.format('Refit time',refit_time_value,suffix)\n    \n    suffix = 'sec'\n    msg += '{0:>20s}:{1:>10.5f}{2:>4s}\\n'.format('mean_fit_time', cv_res.get('mean_fit_time')[row],suffix)\n    msg += '{0:>20s}:{1:>10.5f}{2:>4s}\\n'.format('std_fit_time',  cv_res.get('std_fit_time')[row],suffix)\n    \n    scorings = gscv.get_params().get('scoring')    \n    if type(scorings) == str:\n        scorings = [scorings]\n        \n    for score in scorings:\n        suffix = ''\n        par_name = 'mean_test_' + score\n        msg += '{0:>20s}:{1:>10.5f}{2:>4s}\\n'.format(par_name, cv_res.get(par_name)[row],suffix)\n        par_name = 'std_test_' + score\n        msg += '{0:>20s}:{1:>10.5f}{2:>4s}\\n'.format(par_name, cv_res.get(par_name)[row],suffix)\n        \n    msg += '{0:_>35}\\n'.format('')\n    msg += '\\n'\n\n    # print best hyperparamters values\n    params = gscv.best_params_\n    msg += '\\n{0:>20s}\\n'.format('# Parameters of best estimator:')\n    msg += '{0:_>91}\\n'.format('')\n    for k,v in params.items():\n        msg += '{0:>70s}:{1:>20s}\\n'.format(k,str(v))\n    msg += '{0:_>91}\\n'.format('')\n\n    msg += '\\n'\n    msg += '{0:_>91}\\n'.format('')\n    # print the best estimator pipeline steps\n    msg += '\\n{0:>20s}\\n'.format('# Best estimator pipeline steps:')\n    for step in gscv.best_estimator_.steps:\n        msg += '{}\\n'.format(str(step))\n    msg += '{0:_>91}\\n'.format('')\n    msg += '\\n'\n\n    print(msg)"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "The parameters below define the grid-search CV configuration\n * Will select the best fit as the one with maximum roc_auc. I choose this matric as is it not too sensitive to data umbablance. \n * Below the data is balanced (class_weights = 'balanced') if the algorithm implementation allows. This is the case for Decision Trees, SVC and logistic regression\n * Will performan a 5-fold grid-search cross-validation to obtain the best combination of the hyper-parameters"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# metrics to track\nscorings = ['roc_auc','f1','accuracy','recall','precision']\n# metric to select the best model\n# refit    = 'f1'\nrefit    = 'roc_auc'\n# k-fold CV\ncv       = 5\n# verbosity\nverbose  = 3"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "In there we define a set of preprocessing steps for treating the data before training the final classifier  \nWill define a pipeline with the preprocessing steps"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "from sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV\n\n# Preprocessing pipeline\npreprocessing = Pipeline([('feature_selector', FeatureSelector()), # Select the features to use\n                          ('categorizer',      Categorizer()),     # Apply categoriation to the categorical variables\n                          ('dummyencorer',     DummyEncoder()),    # Apply on-hot-encoding to the categorical variables\n                          ('standard_scaler',  StandardScaler()),  # Apply standard scaler to all the variables\n                         ])\n\n# Define grid of preprocessing parameters to use in the process of GridSearchCV\npreprocessing_grid = {'feature_selector__columns':  [['Principal','terms','age','education','Gender','weekend'],\n                                                     ['Principal','terms','age','education','Gender','weekend','time_to_pay_days'],\n                                                     ['Principal','terms','age','education','Gender','weekend','dayofweek_pay'],\n                                                     ['Principal','terms','age','education','Gender','weekend','dayofweek_pay','time_to_pay_days'],\n                                                    ],\n                      'dummyencorer__drop_first':   [True,False],\n                     }"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "# K Nearest Neighbor(KNN)\nNotice: You should find the best k to build the model with the best accuracy.  \n**warning:** You should not use the __loan_test.csv__ for finding the best k, however, you can split your train_loan.csv into train and test to find the best __k__."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "from sklearn.neighbors import KNeighborsClassifier\n\n# Take the preprocessing pipeline and add at the end the KNN classifier\npipeline_list = preprocessing.steps.copy()\npipeline_list.append(('knn',KNeighborsClassifier()))\n\nknn_estimator = Pipeline(pipeline_list)\nprint(knn_estimator)\n\n# Add the corresponding parameters to the parameters grid\nknn_grid_params = preprocessing_grid.copy()\nknn_grid_params['knn__n_neighbors'] = [1,2,3,4,5,6,8,10,15]\nprint(knn_grid_params)\n\n# print the number of fits to do\nnparams = count_number_of_fits(knn_grid_params)\nnfits   = nparams*cv\nprint(\"# params combinations to test = {}, nfits = {}\".format(nparams,nfits))"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# Define the grid-searchCV object\nknn_gsCV = GridSearchCV(estimator  = knn_estimator,\n                        param_grid = knn_grid_params,\n                        scoring    = scorings,\n                        refit      = refit,\n                        cv         = cv,\n                        verbose    = verbose\n                       )"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "scrolled": true
            },
            "outputs": [],
            "source": "# Now launch the grid-searchCV optimization\nknn_gsCV.fit(X_train,y_train)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "# Print the summary of the results\nsummary_results(knn_gsCV,\"knn_classifier\")"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "# Decision Tree"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "from sklearn.tree import DecisionTreeClassifier\n\n# Take the preprocessing pipeline and add at the end the Decision Tree classifier\npipeline_list = preprocessing.steps.copy()\npipeline_list.append(('decision_tree',DecisionTreeClassifier()))\n\ndecision_tree_estimator = Pipeline(pipeline_list)\nprint(decision_tree_estimator)\n\n# Add the corresponding parameters to the parameters grid\ndecision_tree_grid_params = preprocessing_grid.copy()\ndecision_tree_grid_params['decision_tree__criterion']    = ['entropy']\n# decision_tree_grid_params['decision_tree__max_depth']    = [2,3,5,8,10,15,20]\ndecision_tree_grid_params['decision_tree__max_depth']    = [10,15,20,25,30]\ndecision_tree_grid_params['decision_tree__random_state'] = [123456]\ndecision_tree_grid_params['decision_tree__class_weight'] = ['balanced',None]\n# decision_tree_grid_params['decision_tree__class_weight'] = ['balanced']\ndecision_tree_grid_params['decision_tree__max_features'] = ['auto']\nprint(decision_tree_grid_params)\n\n# print the number of fits to do\nnparams = count_number_of_fits(decision_tree_grid_params)\nnfits   = nparams*cv\nprint(\"# params combinations to test = {}, nfits = {}\".format(nparams,nfits))"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "decision_tree_gsCV = GridSearchCV(estimator  = decision_tree_estimator,\n                                  param_grid = decision_tree_grid_params,\n                                  scoring    = scorings,\n                                  refit      = refit,\n                                  cv         = cv,\n                                  verbose    = verbose\n                                 )"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "scrolled": true
            },
            "outputs": [],
            "source": "decision_tree_gsCV.fit(X_train,y_train)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "summary_results(decision_tree_gsCV,\"decision_tree_classifier\")"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "# Support Vector Machine"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "from sklearn.svm import SVC\n\npipeline_list = preprocessing.steps.copy()\npipeline_list.append(('svc',SVC()))\n\nsvc_estimator = Pipeline(pipeline_list)\nprint(svc_estimator)\n\nsvc_grid_params = preprocessing_grid.copy()\nsvc_grid_params['svc__C']             = [1.0e-2,1.0e-1,1.0]\nsvc_grid_params['svc__kernel']        = ['linear','rbf']\nsvc_grid_params['svc__gamma']         = ['scale','auto']\n# svc_grid_params['svc__class_weight']  = [None,'balanced']\nsvc_grid_params['svc__class_weight']  = ['balanced']\n\nprint(svc_grid_params)\n\nnparams = count_number_of_fits(svc_grid_params)\nnfits   = nparams*cv\nprint(\"# params combinations to test = {}, nfits = {}\".format(nparams,nfits))"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "svc_gsCV = GridSearchCV(estimator  = svc_estimator,\n                        param_grid = svc_grid_params,\n                        scoring    = scorings,\n                        refit      = refit,\n                        cv         = cv,\n                        verbose    = verbose\n                       )"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "scrolled": true
            },
            "outputs": [],
            "source": "svc_gsCV.fit(X_train,y_train)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "summary_results(svc_gsCV,\"svc_classifier\")"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "# Logistic Regression"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "from sklearn.linear_model import LogisticRegression\n\npipeline_list = preprocessing.steps.copy()\npipeline_list.append(('log_reg',LogisticRegression()))\n\nlog_reg_estimator = Pipeline(pipeline_list)\nprint(log_reg_estimator)\n\nlog_reg_grid_params = preprocessing_grid.copy()\nlog_reg_grid_params['log_reg__penalty']      = ['l1','l2']\nlog_reg_grid_params['log_reg__C']            = [1.0e-3,1.0e-2,1.0e-1,1.0]\n# log_reg_grid_params['log_reg__class_weight'] = [None,'balanced']\nlog_reg_grid_params['log_reg__class_weight'] = ['balanced']\nlog_reg_grid_params['log_reg__random_state'] = [1234567]\nlog_reg_grid_params['log_reg__max_iter']     = [100000]\nlog_reg_grid_params['log_reg__solver']       = ['liblinear']\n\nprint(log_reg_grid_params)\n\nnparams = count_number_of_fits(log_reg_grid_params)\nnfits   = nparams*cv\nprint(\"# params combinations to test = {}, nfits = {}\".format(nparams,nfits))"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "log_reg_gsCV = GridSearchCV(estimator  = log_reg_estimator,\n                            param_grid = log_reg_grid_params,\n                            scoring    = scorings,\n                            refit      = refit,\n                            cv         = cv,\n                            verbose    = verbose\n                           )"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "log_reg_gsCV.fit(X_train,y_train)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "summary_results(log_reg_gsCV,\"logistic_regression\")"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "# Model Evaluation using Test set"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "from sklearn.metrics import jaccard_similarity_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import log_loss\nfrom sklearn.metrics import confusion_matrix"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "First, download and load the test set:"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "!wget -O loan_test.csv https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/ML0101ENv3/labs/loan_test.csv"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "source": "### Load Test set for evaluation "
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "outputs": [],
            "source": "# test_df = pd.read_csv('loan_test.csv')\ntest_df = get_data('loan_test.csv')\ntest_df.head()"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "test_df.shape"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "test_df[\"loan_status\"].value_counts()"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "test_df_prep = do_global_preprocessing(test_df)\ntest_df_prep.head()"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "test_df_prep.shape"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "test_df_prep[\"loan_status\"].value_counts()"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "X_test = test_df_prep\ny_test = X_test.pop(\"loan_status\")"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "model_dict = {\"KNN\":                knn_gsCV.best_estimator_,\n              \"Decision Tree\":      decision_tree_gsCV.best_estimator_,\n              \"SVM\":                svc_gsCV.best_estimator_,\n              \"LogisticRegression\": log_reg_gsCV.best_estimator_,\n             }"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "model_perf_dict = {}\ncounter = 0\nfor k,model in model_dict.items():\n    if counter == 0:\n        model_perf_dict[\"Algorithm\"] = []\n        model_perf_dict[\"Jaccard\"]   = []\n        model_perf_dict[\"F1-score\"]  = []\n        model_perf_dict[\"LogLoss\"]   = []\n    \n    model = model_dict.get(k)\n    y_pred = model.predict(X_test)\n    y_prob = None\n    if k == \"LogisticRegression\":\n        y_prob = model.predict_proba(X_test)\n        # print(y_prob)\n        \n    tn, fp, fn, tp = confusion_matrix(y_test,y_pred).ravel()\n    \n    Jaccard = jaccard_similarity_score(y_test,y_pred)\n    F1      = f1_score(y_test,y_pred,pos_label=1)\n    logloss = None\n    if y_prob is not None:\n        logloss = log_loss(y_test,y_prob)\n        \n    precision = tp/(tp + fp)\n    recall    = tp/(tp + fn)\n    fpr       = fp/(fp + tn)\n    \n    print()\n    print()\n    print(\"   PERFORMANCES OF MODEL {}\".format(k))\n    print()\n    print(\"                  True-PAIDOFF    True-COLLECTION\")\n    print(\"                  -------------------------------\")\n    print(\"Pred-PAIDOFF         {}                 {}           =>  {}\".format(tp,fp,tp+fp))\n    print(\"Pred-COLLECTION      {}                 {}           =>  {}\".format(fn,tn,fn+tn))\n    print(\"                  -------------------------------\")\n    print(\"                     {}                 {}\".format(tp+fn,fp+tn))\n    print()\n    print(\"tp        = {}\".format(tp))\n    print(\"fp        = {}\".format(fp))\n    print(\"tn        = {}\".format(tn))\n    print(\"fn        = {}\".format(fn))\n    print(\"F1        = {}\".format(F1))\n    print(\"recall    = {}\".format(recall))\n    print(\"precision = {}\".format(precision))\n    print(\"fpr       = {}\".format(fpr))\n    print()\n        \n    model_perf_dict[\"Algorithm\"].append(k)\n    model_perf_dict[\"Jaccard\"].append(Jaccard)\n    model_perf_dict[\"F1-score\"].append(F1)\n    model_perf_dict[\"LogLoss\"].append(logloss)\n        \n    counter += 1\n    \nperfs = pd.DataFrame.from_dict(model_perf_dict)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": "perfs"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "# Report\nYou should be able to report the accuracy of the built model using different evaluation metrics:"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "| Algorithm          | Jaccard | F1-score | LogLoss |\n|--------------------|---------|----------|---------|\n| KNN                | ?       | ?        | NA      |\n| Decision Tree      | ?       | ?        | NA      |\n| SVM                | ?       | ?        | NA      |\n| LogisticRegression | ?       | ?        | ?       |"
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "button": false,
                "new_sheet": false,
                "run_control": {
                    "read_only": false
                }
            },
            "source": "<h2>Want to learn more?</h2>\n\nIBM SPSS Modeler is a comprehensive analytics platform that has many machine learning algorithms. It has been designed to bring predictive intelligence to decisions made by individuals, by groups, by systems \u2013 by your enterprise as a whole. A free trial is available through this course, available here: <a href=\"http://cocl.us/ML0101EN-SPSSModeler\">SPSS Modeler</a>\n\nAlso, you can use Watson Studio to run these notebooks faster with bigger datasets. Watson Studio is IBM's leading cloud solution for data scientists, built by data scientists. With Jupyter notebooks, RStudio, Apache Spark and popular libraries pre-packaged in the cloud, Watson Studio enables data scientists to collaborate on their projects without having to install anything. Join the fast-growing community of Watson Studio users today with a free account at <a href=\"https://cocl.us/ML0101EN_DSX\">Watson Studio</a>\n\n<h3>Thanks for completing this lesson!</h3>\n\n<h4>Author:  <a href=\"https://ca.linkedin.com/in/saeedaghabozorgi\">Saeed Aghabozorgi</a></h4>\n<p><a href=\"https://ca.linkedin.com/in/saeedaghabozorgi\">Saeed Aghabozorgi</a>, PhD is a Data Scientist in IBM with a track record of developing enterprise level applications that substantially increases clients\u2019 ability to turn data into actionable knowledge. He is a researcher in data mining field and expert in developing advanced analytic methods like machine learning and statistical modelling on large datasets.</p>\n\n<hr>\n\n<p>Copyright &copy; 2018 <a href=\"https://cocl.us/DX0108EN_CC\">Cognitive Class</a>. This notebook and its source code are released under the terms of the <a href=\"https://bigdatauniversity.com/mit-license/\">MIT License</a>.</p>"
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.6",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.6.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}